{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7926730",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: Logistic Regression and Naive Bayes\n",
    "\n",
    "In this lesson, students will be introduced to the following concepts:\n",
    "- Basics of Machine Learning,\n",
    "- A simple pipeline of a typical machine learning project,\n",
    "- A guided tour through a small sentence classification task that will incorporate:\n",
    "    - NLTK Text Preprocessing\n",
    "    - Logistic Regression\n",
    "    - Naive Bayes\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "\n",
    "Machine learning is a branch of artificial intelligence that involves building algorithms and statistical models that allow computers to learn from and make predictions or decisions based on data. The idea is to enable computers to identify patterns and relationships in data, and then use that knowledge to make predictions or decisions about new data. These algorithms can assist in processing and analyzing endless amounts of data that would normally be impossible to comprehend by human minds.\n",
    "\n",
    "Machine learning is a rapidly growing field with a wide range of applications, including:\n",
    "- image recognition\n",
    "- natural language processing\n",
    "- speech recognition\n",
    "- recommendation systems\n",
    "- Stock Price prediction\n",
    "- Object detection systems\n",
    "\n",
    "There are three primary types of machine learning algorithms:\n",
    "- **Supervised learning**: The model is trained on data that is labelled i.e. each sample being categorized, before being made to predict unseen data. Supervised learning can be further divided into two types of prediction tasks:\n",
    "    - <i>Classification</i>: The model is made to put each sample under one or more specific class labels, with the output being discrete. Eg: Student grade prediction, emotion recognition, semantic analysis, etc. Classification tasks can be of **Binary (classifying from two labels), or Multi-classification (classifying from multiple labels)**.\n",
    "    - <i>Regression</i>: The model needs to predict a continuous output. Eg: Temperature, stock market prices, etc.\n",
    "- **Unsupervised learning**: Unlabelled data is used to train the model, forcing it to self-extract patterns before prediction of unseen data.\n",
    "- **Reinforcement learning**: During the process of training the data, a 'points' system is implemented wherein the model is rewarded for making correct predictions and penalized for incorrect ones.\n",
    "\n",
    "Machine learning has the potential to transform industries and change the way we live and work. With it being an integral aspect of Natural Language Processing, it is important to become familiar with the fundamentals of machine learning in order to grasp future concepts that will be taught in this course.\n",
    "\n",
    "For now, this lesson will not only serve as a guide to a basic procedure for preparing machine learning data, but also as an introduction to two popular supervised learning algorithms: \n",
    "- **Logistic Regression**\n",
    "- **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edd560",
   "metadata": {},
   "source": [
    "## The Basic Pipeline of a Machine Learning Project\n",
    "\n",
    "For any machine learning task, a model needs to be familiarized, or **trained** with the type of information it is being made to predict, hence requiring a substantial amount of data. Data exists in numerous forms, from videos and images to tables and audio clips. One cannot directly take this raw, unprocessed information and 'stuff' it into the model, expecting it to train like magic. This is where **data preprocessing** comes into play, in which information is cleaned, re-arranged, and converted for the model to easily read and train on without much noise/bias. These aforementioned concepts will be discussed further into this lesson.\n",
    "\n",
    "A typical pipeline of a machine learning model involves:\n",
    "1. Determining whether the task involves classification or regression, and collecting data based on the problem.\n",
    "2. Data preprocessing: data cleaning, text preprocessing, omitting irrelevant information, format conversion, etc.\n",
    "3. Determining the portions of data that will be used for:\n",
    "    - **TRAINING**: The data samples that are fed to the model to familiarize itself with the type of problem and patterns it is expected to identify.\n",
    "    - **VALIDATION**: (optional but highly recommended) During the training process, the model can be made to predict samples belonging to a 'validation' set, in order to evaluate the prediction performance and check room for improvement.\n",
    "    - **TESTING**: The portion of data that is not known at all by the model, and will be used as the final measure of performance after the training/validation process is complete.\n",
    "4. Tuning hyperparameters/settings of the machine learning algorithms,\n",
    "5. Training and testing the models\n",
    "6. Evaluation; if performance needs to be improved, repeat steps 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a0cfc",
   "metadata": {},
   "source": [
    "## I. Our First NLP Task: Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the process of using natural language processing to extract subjective information from text/speech information. The goal of sentiment analysis is to determine the attitude, emotion, or opinion expressed in a piece of text, such as a social media post, customer review, or news article. \n",
    "\n",
    "Sentiment analysis algorithms typically use machine learning or deep learning techniques to analyze the text, which can involve identifying specific words, phrases, or patterns that are associated with positive, negative, or neutral sentiment. The output of sentiment analysis is often a numerical score or label that reflects the sentiment of the text, ranging from very negative to very positive, or neutral. \n",
    "\n",
    "Sentiment analysis is useful in a variety of applications, including market research, brand management, customer service, and political analysis, among others. It can provide valuable insights into public opinion, consumer behavior, and other trends, and can be used to inform decision-making and strategy development.\n",
    "\n",
    "In this section, we will be using a dataset containing Amazon reviews for training 2 models and trying to predict the sentiment of each review, making this task a **SENTENCE CLASSIFICATION** problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0db57",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries\n",
    "\n",
    "For our classification task, the following libraries will be utilized:\n",
    " - **pandas**: For dataset manipulation\n",
    " - **re**: REGEX tool for text preprocessing\n",
    " - **NLTK**: A vast library containing tools for text preprocessing, language modelling, vectorization, and tokenization.\n",
    " - **sklearn**: A machine learning library that consists of dozens of algorithms to experiment with, including Naive Bayes and Logistic Regression\n",
    " - **numpy**: An array manipulation library, consisting of tools for performing various array operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ba0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb26bd",
   "metadata": {},
   "source": [
    "### 2. Loading the Amazon Reviews Sentiment Dataset\n",
    "**Source**: https://www.kaggle.com/datasets/tarkkaanko/amazon\n",
    "#### Dataset Information:\n",
    "- A dataset containing about 4900 customer reviews for products bought through Amazon, with a rating assigned to each of them. Other features concerning the username, time of rating, population stats, etc. are also present.\n",
    "\n",
    "- The dataset seems to be in a csv format, so we use pandas's **read_csv()** function to read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6781b531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>day_diff</th>\n",
       "      <th>helpful_yes</th>\n",
       "      <th>helpful_no</th>\n",
       "      <th>total_vote</th>\n",
       "      <th>score_pos_neg_diff</th>\n",
       "      <th>score_average_rating</th>\n",
       "      <th>wilson_lower_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No issues.</td>\n",
       "      <td>2014-07-23</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0mie</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Purchased this for my device, it worked as adv...</td>\n",
       "      <td>2013-10-25</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1K3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>it works as expected. I should have sprung for...</td>\n",
       "      <td>2012-12-23</td>\n",
       "      <td>715</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1m2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This think has worked out great.Had a diff. br...</td>\n",
       "      <td>2013-11-21</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2&amp;amp;1/2Men</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought it with Retail Packaging, arrived legit...</td>\n",
       "      <td>2013-07-13</td>\n",
       "      <td>513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  reviewerName  overall   \n",
       "0           0           NaN      4.0  \\\n",
       "1           1          0mie      5.0   \n",
       "2           2           1K3      4.0   \n",
       "3           3           1m2      5.0   \n",
       "4           4  2&amp;1/2Men      5.0   \n",
       "\n",
       "                                          reviewText  reviewTime  day_diff   \n",
       "0                                         No issues.  2014-07-23       138  \\\n",
       "1  Purchased this for my device, it worked as adv...  2013-10-25       409   \n",
       "2  it works as expected. I should have sprung for...  2012-12-23       715   \n",
       "3  This think has worked out great.Had a diff. br...  2013-11-21       382   \n",
       "4  Bought it with Retail Packaging, arrived legit...  2013-07-13       513   \n",
       "\n",
       "   helpful_yes  helpful_no  total_vote  score_pos_neg_diff   \n",
       "0            0           0           0                   0  \\\n",
       "1            0           0           0                   0   \n",
       "2            0           0           0                   0   \n",
       "3            0           0           0                   0   \n",
       "4            0           0           0                   0   \n",
       "\n",
       "   score_average_rating  wilson_lower_bound  \n",
       "0                   0.0                 0.0  \n",
       "1                   0.0                 0.0  \n",
       "2                   0.0                 0.0  \n",
       "3                   0.0                 0.0  \n",
       "4                   0.0                 0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_reviews.csv')\n",
    "### Viewing the first 5 rows of the dataset to get a bettere look at it.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0d8f7",
   "metadata": {},
   "source": [
    "We can view the dimensions of the dataset through the **shape** call.\n",
    "- The first number in the output represents the number of rows, followed by the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860455e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4915, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445fcd9a",
   "metadata": {},
   "source": [
    "### 3. Removing irrelevant data columns and empty rows\n",
    "From the cell above, we get to know that the dataset contains 4915 review samples, and 12 features. \n",
    "\n",
    "- For this task, we are only interested in the 'overall' (which is the overall product rating), and 'reviewText' features.\n",
    "- Let's slice the dataset through **df.loc** call, putting our focus only on the two aforementioned columns. We can also use the **df.drop** call to specify column names to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e08f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined = df.loc[:,'overall':'reviewText']\n",
    "df_refined = df_refined.dropna()\n",
    "df_refined = df_refined.reset_index()\n",
    "df_refined = df_refined.loc[:,'overall':'reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92c9d809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>No issues.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Purchased this for my device, it worked as adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>it works as expected. I should have sprung for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This think has worked out great.Had a diff. br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought it with Retail Packaging, arrived legit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>It's mini storage.  It doesn't do anything els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have it in my phone and it never skips a bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>It's hard to believe how affordable digital ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Works in a HTC Rezound.  Was running short of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>in my galaxy s4, super fast card, and am total...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText\n",
       "0      4.0                                         No issues.\n",
       "1      5.0  Purchased this for my device, it worked as adv...\n",
       "2      4.0  it works as expected. I should have sprung for...\n",
       "3      5.0  This think has worked out great.Had a diff. br...\n",
       "4      5.0  Bought it with Retail Packaging, arrived legit...\n",
       "5      5.0  It's mini storage.  It doesn't do anything els...\n",
       "6      5.0  I have it in my phone and it never skips a bea...\n",
       "7      5.0  It's hard to believe how affordable digital ha...\n",
       "8      5.0  Works in a HTC Rezound.  Was running short of ...\n",
       "9      5.0  in my galaxy s4, super fast card, and am total..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Viewing the sliced Dataset we will be working on:\n",
    "df_refined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad7ea6",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "To gain a better frame of understanding of the information you are dealing with, it is important to perform analysis on the dataset. If one would like to take a look at the amount of products associated with each rating, we could use a plotting library like **seaborn** to get a good visual of the sample frequency of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d047d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='overall', ylabel='count'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=df_refined[\"overall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b293bb",
   "metadata": {},
   "source": [
    "From the visual above, we witness that the amount of 5 star products is much greater than that of the other ratings, resulting in an **imbalanced** dataset. An imbalanced dataset can lead to a machine learning phenomenon known as **overfitting**, wherein the model gets fed too many samples of one class, and ends up memorizing only the patterns associated with that class and nothing else. \n",
    "\n",
    "- We will later see how this data imbalance could potentially play a role in the model performance metrics after the testing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a2c84",
   "metadata": {},
   "source": [
    "### 4. Converting Numeric Ratings to Sentiment Categories\n",
    "\n",
    "- The ratings in the 'overall' column represent a discrete label that come under '1.0', '2.0', '3.0', '4.0', '5.0' classes. \n",
    "- As we are mostly interested in the **sentiment** of the product rather than the specific rating, we want to further simplify these labels into **'Positive' and 'Negative'** classes.\n",
    "- We use a conditional 'filter' to convert each rating to one of the aforementioned classes:\n",
    "    - 1.0, 2.0 are converted to 'Negative'\n",
    "    - 3.0, 4.0, and 5.0 are converted to 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb84cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting the rating labels\n",
    "df_refined['overall'] = np.where(df_refined['overall'] == '5.0', 'Positive', df_refined['overall'])\n",
    "df_refined['overall'] = np.where(df_refined['overall'] == '4.0', 'Positive', df_refined['overall'])\n",
    "df_refined['overall'] = np.where(df_refined['overall'] == '3.0', 'Positive', df_refined['overall'])\n",
    "\n",
    "df_refined['overall'] = np.where(df_refined['overall'] == '2.0', 'Negative', df_refined['overall'])\n",
    "df_refined['overall'] = np.where(df_refined['overall'] == '1.0', 'Negative', df_refined['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bee4707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>No issues.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Purchased this for my device, it worked as adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>it works as expected. I should have sprung for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This think has worked out great.Had a diff. br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Bought it with Retail Packaging, arrived legit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>It's mini storage.  It doesn't do anything els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have it in my phone and it never skips a bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>It's hard to believe how affordable digital ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Works in a HTC Rezound.  Was running short of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>in my galaxy s4, super fast card, and am total...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    overall                                         reviewText\n",
       "0  Positive                                         No issues.\n",
       "1       5.0  Purchased this for my device, it worked as adv...\n",
       "2  Positive  it works as expected. I should have sprung for...\n",
       "3       5.0  This think has worked out great.Had a diff. br...\n",
       "4       5.0  Bought it with Retail Packaging, arrived legit...\n",
       "5       5.0  It's mini storage.  It doesn't do anything els...\n",
       "6       5.0  I have it in my phone and it never skips a bea...\n",
       "7       5.0  It's hard to believe how affordable digital ha...\n",
       "8       5.0  Works in a HTC Rezound.  Was running short of ...\n",
       "9       5.0  in my galaxy s4, super fast card, and am total..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1545d",
   "metadata": {},
   "source": [
    "### 5. Text Preprocessing\n",
    "\n",
    "In order to reduce bias, chances of overfitting and confusion within the models, we want to simplify the review text through:\n",
    "- Converting to lowercase\n",
    "- Removing stopwords such as 'the', 'a', 'an', 'in', 'on', etc. to reduce noise\n",
    "- Lemmatization/Stemming: Simplifying the word to its root. Example: 'Running' simplifies to the root word 'run'.\n",
    "- Removing punctuation through REGEX\n",
    "- tokenization: Splitting text into words\n",
    "\n",
    "Of course, there is no hard-and-fast rule as to how text should be preprocessed. In fact, it is very possible that not performing one or more of the aforementioned steps can lead to better results, depending on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f69dd",
   "metadata": {},
   "source": [
    "### Importing NLTK stopwords and Lemmatization data\n",
    "Using the stopwords/Lemma in the NLTK database as a reference, we can simplify our review texts to reduce potential confusion/overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e87c663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65442ea",
   "metadata": {},
   "source": [
    "### Preprocessing begins\n",
    "\n",
    "- Now we perform the stopword/punctuation removal, along with lemmatization of the words.\n",
    "- This process may take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6086385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/home/codespace/.python/current/nltk_data'\n    - '/home/codespace/.python/current/share/nltk_data'\n    - '/home/codespace/.python/current/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m review \u001b[39m=\u001b[39m review\u001b[39m.\u001b[39mlower() \n\u001b[1;32m      7\u001b[0m review \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, review)\n\u001b[0;32m----> 8\u001b[0m tokenList \u001b[39m=\u001b[39m word_tokenize(review)\n\u001b[1;32m     10\u001b[0m \u001b[39m##Lemmatizer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/home/codespace/.python/current/nltk_data'\n    - '/home/codespace/.python/current/share/nltk_data'\n    - '/home/codespace/.python/current/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "### Text Preprocessing\n",
    "df_refined['reviewText'] \n",
    "\n",
    "for i in range(len(df_refined['reviewText'])) :\n",
    "    review = df_refined.loc[i, 'reviewText']\n",
    "    review = review.lower() \n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    tokenList = word_tokenize(review)\n",
    "    \n",
    "    ##Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokenList]\n",
    "\n",
    "    tokenList2 = [word for word in lemmatized_tokens if not word in stopwords.words()]\n",
    "    df_refined.loc[i, 'reviewText'] = ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f62c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f75c6f",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training, Validation and Test sets\n",
    "\n",
    "- We use sklearn's **train_test_split** to achieve a 70/15/15 train-validation-test split on our dataset.\n",
    "- This means that 70% of the 4915 samples will be training samples, 15% for validation, and another 15% will be the unseen test samples.\n",
    "- Our feature will be the review text, while the labels will be the values in the 'overall' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_refined['reviewText']\n",
    "y = df_refined['overall']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a0a12b",
   "metadata": {},
   "source": [
    "### 6. Vectorization: Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aaef1e",
   "metadata": {},
   "source": [
    "- As the reviews are written in natural human language, the words would need to be converted into numerical features for the model to understand. This process is also known as **vectorization**.\n",
    "\n",
    "- Several Vectorization techniques exist depending on the task at hand, considering factors like semantic context, position, frequency, connotation, etc. to convert each word into a unique vector.\n",
    "\n",
    "- For our task, the **Bag of Words (BOW)** model will be utilized for sentiment analysis. The BOW technique is a vectorization method that splits up each sentence or portion of text into a 'bag' of words, in which each word represents an individual feature of the produced vector. The frequency of each word within the text corresponds to the value within the vector itself.\n",
    "\n",
    "- We will use Sklearn's **CountVectorizer** function, which functions the same as a Bag of Words algorithm, to convert each sentence into vectors to be fed to the models.\n",
    "    - **NOTE**: When it comes to vectorization, the vectorizing model should only be fitted onto the training data, and NOT the test data. Since the test data is completely unknown to us and thus cannot be analyzed for numeric conversion by the model upon feeding, we vectorize the unseen data from the information the model has by fitting on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244842c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit(X_train)\n",
    "\n",
    "training_data = bow.transform(X_train)\n",
    "val_data = bow.transform(X_val)\n",
    "test_data = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09c223",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365aa3a",
   "metadata": {},
   "source": [
    "### 7. Logistic Regression\n",
    "\n",
    "- Logistic regression is a statistical technique used to model the relationship between a binary dependent variable and one or more independent variables. It is a type of regression analysis commonly used in various fields, including business, medicine, psychology, and social sciences, to predict the probability of a certain event occurring.\n",
    "\n",
    "- The primary goal of logistic regression is to determine the relationship between the dependent variable (the output) and the independent variables (features) by estimating the probability of the dependent variable taking a certain value, given a specific set of values of the independent variables. The dependent variable in logistic regression is dichotomous, meaning that it can only take on two possible values, such as yes/no or success/failure.\n",
    "\n",
    "- Logistic regression uses a mathematical function called the **logistic function or sigmoid** function to model the relationship between the independent variables and the dependent variable. The output of the logistic regression model is a probability score ranging from 0 to 1, which represents the likelihood of the dependent variable taking a certain value.\n",
    "\n",
    "- Thus, despite the name implications, **Logistic Regression** is usually seen as a **classification model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b18a9",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189beef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', dual=False, tol=0.0001,\n",
    "                       C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs',\n",
    "                       max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "lrFit = lr.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1e59c",
   "metadata": {},
   "source": [
    "#### Validating the model and analyzing the accuracy and precision metrics\n",
    "\n",
    "- **Accuracy**: A measure of the portion of correct prediction made on the dataset, compared to the original labelled data.\n",
    "- **Precision**: A measure of how close (or precise) the predictions are to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352db66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrFit.predict(val_data)\n",
    "print('Accuracy: ' ,accuracy_score(y_val, predictions))\n",
    "print('Precision: ',precision_score(y_val,predictions, pos_label=\"Positive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed413b",
   "metadata": {},
   "source": [
    "It seems that the metrics achieved on the validation set are high enough for us to move on to evaluating the test set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c0ed6",
   "metadata": {},
   "source": [
    "#### Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5467e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lrFit.predict(test_data)\n",
    "print('Accuracy: ' ,accuracy_score(y_test, test_predictions))\n",
    "print('Precision: ',precision_score(y_test , test_predictions, pos_label=\"Positive\"))\n",
    "\n",
    "cm = confusion_matrix(y_test, test_predictions, labels=lrFit.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lrFit.classes_)\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860b96e",
   "metadata": {},
   "source": [
    "### 8. Naive Bayes and Gaussian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac2679",
   "metadata": {},
   "source": [
    "- **Naive Bayes** is a simple and efficient algorithm that is commonly used in natural language processing, spam filtering, and sentiment analysis. It is based on the assumption that the presence or absence of a feature is independent of the presence or absence of any other feature, hence the term \"naive\". The algorithm calculates the probability of each class given the input features and selects the class with the highest probability as the output.\n",
    "\n",
    "- **Gaussian Classifier** is a type of Naive Bayes algorithm that assumes that the probability distribution of the input features is Gaussian (normal). This means that the algorithm assumes that the features follow a bell-shaped curve, which is a common assumption in many real-world scenarios. Gaussian Classifier is often used in image recognition and computer vision tasks, where the features can be represented as pixel values, and the assumption of Gaussian distribution can be reasonable.\n",
    "\n",
    "- The Gaussian Classifier calculates the mean and variance of each feature for each class and uses them to calculate the probability of each class given the input features. It assumes that the probability distribution of each feature in each class is Gaussian and calculates the probability of the feature given the class using the mean and variance of the feature in that class. The algorithm then multiplies the probabilities of all the features to calculate the probability of the input belonging to each class and selects the class with the highest probability as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66df079",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB = GaussianNB()\n",
    "GBFit = GB.fit(training_data.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a9560",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27242181",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = GBFit.predict(val_data.toarray())\n",
    "print('Accuracy: ' ,accuracy_score(y_val, predictions))\n",
    "print('Precision: ',precision_score(y_val,predictions, pos_label=\"Positive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d59b7",
   "metadata": {},
   "source": [
    "#### Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = GBFit.predict(test_data.toarray())\n",
    "print('Accuracy: ' ,accuracy_score(y_test, test_predictions))\n",
    "print('Precision: ',precision_score(y_test , test_predictions, pos_label=\"Positive\"))\n",
    "\n",
    "cm = confusion_matrix(y_test, test_predictions, labels=GBFit.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=GBFit.classes_)\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6eae6b",
   "metadata": {},
   "source": [
    "# FOOD FOR THOUGHT\n",
    "\n",
    "1. Looking at the confusion matrices above, do you think these results are a good indicator of the models' performance? Why or Why not?\n",
    "2. Search for alternative methods that are used to reduce overfitting and data imbalance.\n",
    "3. Could Logistic Regression and Gaussian classifier be used for Regression tasks?\n",
    "4. Compare and contrast the scores of the Log regression and Gaussian models. How could the Gaussian model's accuracy be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c4bb4",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "By the end of this lesson, you have hopefully grasped some of the base concepts of the machine learning process. With this being the stepping stone to a much wider realm of knowledge, the next chapters will delve into the concepts of a more complex and powerful aspect of Artificial intelligence that focuses on trying to emulate the human brain itself: **Deep Learning**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
